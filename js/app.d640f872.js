(function(e){function t(t){for(var a,r,s=t[0],c=t[1],h=t[2],g=0,l=[];g<s.length;g++)r=s[g],Object.prototype.hasOwnProperty.call(o,r)&&o[r]&&l.push(o[r][0]),o[r]=0;for(a in c)Object.prototype.hasOwnProperty.call(c,a)&&(e[a]=c[a]);p&&p(t);while(l.length)l.shift()();return i.push.apply(i,h||[]),n()}function n(){for(var e,t=0;t<i.length;t++){for(var n=i[t],a=!0,s=1;s<n.length;s++){var c=n[s];0!==o[c]&&(a=!1)}a&&(i.splice(t--,1),e=r(r.s=n[0]))}return e}var a={},o={app:0},i=[];function r(t){if(a[t])return a[t].exports;var n=a[t]={i:t,l:!1,exports:{}};return e[t].call(n.exports,n,n.exports,r),n.l=!0,n.exports}r.m=e,r.c=a,r.d=function(e,t,n){r.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:n})},r.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},r.t=function(e,t){if(1&t&&(e=r(e)),8&t)return e;if(4&t&&"object"===typeof e&&e&&e.__esModule)return e;var n=Object.create(null);if(r.r(n),Object.defineProperty(n,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var a in e)r.d(n,a,function(t){return e[t]}.bind(null,a));return n},r.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return r.d(t,"a",t),t},r.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},r.p="";var s=window["webpackJsonp"]=window["webpackJsonp"]||[],c=s.push.bind(s);s.push=t,s=s.slice();for(var h=0;h<s.length;h++)t(s[h]);var p=c;i.push([0,"chunk-vendors"]),n()})({0:function(e,t,n){e.exports=n("cd49")},"0554":function(e,t,n){e.exports=n.p+"img/0.3cffc5ad.png"},"064c":function(e,t,n){e.exports=n.p+"img/1.17b8ddf2.png"},"071d":function(e,t,n){"use strict";n("14e4")},"0b40":function(e,t,n){e.exports=n.p+"img/4.492ef0eb.png"},"0fa9":function(e,t,n){e.exports=n.p+"img/6.8a31a92b.png"},"14e4":function(e,t,n){},"19f8":function(e,t,n){},"1ab2":function(e,t,n){e.exports=n.p+"img/3.e5224c07.png"},"1dd2":function(e,t,n){"use strict";n("766e")},2155:function(e,t,n){e.exports=n.p+"img/NUS.e8930f16.jpg"},"22e7":function(e,t,n){var a={"./NUS.jpg":"2155","./TORONTO.jpg":"ccb7","./清华大学.jpg":"23f8","./香港科技大学.jpg":"297f"};function o(e){var t=i(e);return n(t)}function i(e){if(!n.o(a,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return a[e]}o.keys=function(){return Object.keys(a)},o.resolve=i,e.exports=o,o.id="22e7"},"23f8":function(e,t,n){e.exports=n.p+"img/清华大学.c111f20f.jpg"},"297f":function(e,t,n){e.exports=n.p+"img/香港科技大学.e64963c4.jpg"},"2e45":function(e,t,n){},3410:function(e,t,n){},3651:function(e,t,n){},"3a4d":function(e,t,n){"use strict";n("3651")},"3c15":function(e,t,n){"use strict";n("19f8")},"47c3":function(e,t,n){"use strict";n("2e45")},"48b1":function(e,t,n){},"48bc":function(e,t,n){"use strict";n("5d5d")},"55ba":function(e,t,n){},"5d3f":function(e,t,n){"use strict";n("bdb4")},"5d5d":function(e,t,n){},"63bf":function(e,t,n){},6645:function(e,t,n){},"6b3b":function(e,t,n){"use strict";n("3410")},"73d7":function(e,t){e.exports="data:image/gif;base64,R0lGODlhHAALAJECAAAAAP/WAP///wAAACH/C05FVFNDQVBFMi4wAwEAAAAh+QQJFAACACwAAAAAHAALAAACO5SPqcutAZ1EAYQDb165i3oZIGR1nYVqFgaUbtuS4LqJqWvO4Lfa5AuTpT4Rn2xkSoUSmxOM5pmIctICACH5BAkUAAIALAAAAAAcAAsAAAI8lI+pyxIPQ0vQQYBxjFaaAFxPuIWmBHqOloIZeYarmp5tFKPyB/fZWEM9DjfSRkSaEYeVxeYznEin1EkBACH5BAkUAAIALAAAAAAcAAsAAAI3lI+py70BnUQBhARv3lnVS1mQCJQll1nY+bWbBXfhR5ok/a3jzcUieHDVTCPcI4JCxYCTo8xQAAAh+QQFFAACACwAAAAAHAALAAACOZSPqcvtF6KIYdFrAdX36o9BAEeOYwSSlRiAbfe6GvK2pYnGOc3VvVej0CQe06wzmTlgq4TwAVUUAAA7"},"766e":function(e,t,n){},"86ea":function(e,t,n){"use strict";n("48b1")},"8aa4":function(e,t,n){},"8c41":function(e,t,n){"use strict";n("55ba")},9278:function(e,t,n){e.exports=n.p+"img/5.cf638615.png"},"94db":function(e,t,n){},"9af6":function(e,t,n){"use strict";n("8aa4")},aa33:function(e,t,n){},ad3d7:function(e,t,n){"use strict";n("6645")},bdb4:function(e,t,n){},c7b0:function(e,t,n){e.exports=n.p+"img/2.e01b6503.png"},ccb7:function(e,t,n){e.exports=n.p+"img/TORONTO.084008c3.jpg"},cd49:function(e,t,n){"use strict";n.r(t);var a=n("2b0e"),o=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{attrs:{id:"app"}},[t("Header"),t("div",{staticClass:"container href",attrs:{id:"Home"}},[t("Profile"),t("info-list"),t("NavigationBar"),t("New"),t("ResearchInterest"),t("Publication"),t("Education"),t("ProfessionalExperience"),t("Honors"),t("Patent"),t("SoftwareCopyrght"),t("ScientificFund"),t("Footer")],1)],1)},i=[],r=n("9ab4"),s=n("1b40"),c=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"header-wrapper"},[t("div",{staticClass:"content-wrapper"},[t("div",{staticClass:"left"},[t("div",{staticClass:"name"},[t("span",[e._v(e._s(e.words.name))])]),t("div",{staticClass:"blog"},[e._v(e._s(e.words.blog))])]),t("div",{staticClass:"right"},e._l(e.LanguageItems,(function(n,a){return t("div",{key:a,staticClass:"language-item",on:{click:()=>e.changeLanguage(n)}},[e._v(" "+e._s(n.__identity)+" ")])})),0)])])},h=[],p=(n("e9f5"),n("ab43"),n("4bb5"));let g=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e])}changeLanguage(e){this.$store.dispatch("setLanguage",e.__langKey)}};Object(r["a"])([Object(s["b"])()],g.prototype,"msg",void 0),Object(r["a"])([Object(p["a"])("words")],g.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],g.prototype,"dictionary",void 0),g=Object(r["a"])([s["a"]],g);var l=g,u=l,d=(n("9af6"),n("2877")),f=Object(d["a"])(u,c,h,!1,null,"7eb63195",null),m=f.exports,w=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"info-list-wrapper"},[e._l(e.longInfoList,(function(n,a){return t("div",{key:a,staticClass:"long-info-list list-item"},[t("font-awesome-icon",{staticClass:"icon",attrs:{icon:n.icon}}),e._v(" "+e._s(n.msg)+" ")],1)})),t("div",{staticClass:"short-list-wrapper"},e._l(e.shortInfoList,(function(n,a){return t("div",{key:a,staticClass:"short-info-list list-item"},[t("font-awesome-icon",{staticClass:"icon",attrs:{icon:n.icon}}),e._v(" "+e._s(n.msg)+" ")],1)})),0),t("div",{staticClass:"icons"},e._l(e.icons,(function(n,a){return t("div",{key:a,staticClass:"icon"},[t("a",{staticStyle:{display:"block"},attrs:{href:n.href}},[t("font-awesome-icon",{staticClass:"icon",style:{color:n.hovTheme,backgroundColor:n.bgC},attrs:{icon:n.icon},on:{mouseenter:()=>e.setColor(n),mouseleave:()=>e.resetColor(n)}})],1)])})),0)],2)},b=[];let y=class extends s["c"]{constructor(){super(...arguments),this.icons=[{hovTheme:"#000",icon:["fab","github"],href:"https://github.com/lin-nie",theme:"rgb(255, 0, 0)"},{hovTheme:"#000",icon:["fab","google"],href:"https://scholar.google.com/citations?user=QNRj_g8AAAAJ&hl=en",theme:"rgb(67, 135, 246)"},{hovTheme:"#000",icon:["fab","linkedin"],href:"https://www.linkedin.com/in/nie-lin/",theme:"rgb(0, 127, 178)"},{hovTheme:"#000",icon:["fab","twitter-square"],href:"https://twitter.com/NieLin6",theme:"rgb(29, 155, 240)"}]}created(){}get longInfoList(){return[{icon:["fas","home"],msg:this.words.address}]}get shortInfoList(){return[{icon:["fas","envelope"],msg:this.words.email},{icon:["fas","phone-alt"],msg:this.words.phone},{icon:["fas","tv"],msg:this.words.web}]}setColor(e){e.hovTheme=e.theme}resetColor(e){e.hovTheme="#000",e.bgC="#fff"}};Object(r["a"])([Object(p["a"])("words")],y.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],y.prototype,"dictionary",void 0),y=Object(r["a"])([s["a"]],y);var v=y,_=v,C=(n("d9ea"),Object(d["a"])(_,w,b,!1,null,"6440070c",null)),S=C.exports,P=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"profile-wrapper"},[t("div",{staticClass:"profile-info-wrapper"},[t("div",{staticClass:"profile-image"},[t("img",{attrs:{src:n("f5f6")}}),t("ul",{staticClass:"content"},[t("span",{staticStyle:{display:"inline-block",width:"2ch",height:"1em","background-color":"transparent"}}),t("i",[e._v(e._s(e.words.pictureTime))])])]),t("div",{staticClass:"profile"},[t("h1",{staticClass:"name"},[e._v(" "+e._s(e.words.name)+" ")]),t("p",{staticClass:"advance-line"},[t("a",{attrs:{href:"https://www.u-tokyo.ac.jp/en/prospective-students/fellowship.html"}},[e._v(e._s(e.words.degree))])]),t("p",{staticClass:"advance-line"},[e._v(" "+e._s(e.words.major)+" ")]),t("p",{staticClass:"advance-line"},[e._v(" "+e._s(e.words.department)+" ")]),t("p",{staticClass:"advance-line"},[e._v(" "+e._s(e.words.university)+" ")]),t("hr",{staticStyle:{"margin-top":"5px"}}),t("div",{staticClass:"introduction"},[t("ul",{staticClass:"content"},[t("p",{domProps:{innerHTML:e._s(e.words.personalIntroduction)}})])])])]),t("div",{staticClass:"address-wrapper"}),e._m(0)])},k=[function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"info-wrapper"},[t("div",{staticClass:"infos"}),t("div",{staticClass:"icons"})])}];let j=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],j.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],j.prototype,"dictionary",void 0),j=Object(r["a"])([s["a"]],j);var T=j,A=T,I=(n("d12b"),Object(d["a"])(A,P,k,!1,null,"1f1cf286",null)),L=I.exports,x=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"navigation-wrapper"},[t("ul",[t("p",{staticClass:"name"},[e._v(e._s(e.words.navigation.name))]),e._l(e.words.navigation.address,(function(n,a){return t("li",[t("a",{staticClass:"menu-item",attrs:{href:"#"+e.href[a]}},[e._v(e._s(n))])])}))],2)])},O=[];let N=class extends s["c"]{constructor(){super(...arguments),this.href=["Home","New","Research Interest","Publication","Education","Professional Experience","Honors","Patent","Software Copyrght","Research Funding"]}get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],N.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],N.prototype,"dictionary",void 0),N=Object(r["a"])([s["a"]],N);var E=N,H=E,M=(n("47c3"),Object(d["a"])(H,x,O,!1,null,"483e6130",null)),R=M.exports,U=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"new-wrapper wrapper-style href",attrs:{id:"New"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.newTitle)+" ")])])]),t("ul",{staticClass:"show-list"},[t("li",{domProps:{innerHTML:e._s(e.words.new.new25)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new24)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new23)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new22)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new21)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new20)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new19)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new18)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new17)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new16)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new15)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new14)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new13)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new12)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new11)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new10)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new9)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new8)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new7)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new6)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new5)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new4)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new3)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new2)}}),t("li",{domProps:{innerHTML:e._s(e.words.new.new1)}})])])},F=[];let V=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],V.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],V.prototype,"dictionary",void 0),V=Object(r["a"])([s["a"]],V);var z=V,D=z,G=(n("3a4d"),Object(d["a"])(D,U,F,!1,null,null,null)),J=G.exports,K=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"research-wrapper wrapper-style href",attrs:{id:"Research Interest"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.researchTitle)+" ")])])]),t("ul",{staticClass:"research-content"},[t("Strong",[e._v(e._s(e.words.overallField))]),t("br"),e._l(e.words.researchOverInterest,(function(n){return t("li",[e._v(" "+e._s(n)+" ")])})),t("br"),t("Strong",[e._v(e._s(e.words.specialField))]),t("br"),e._l(e.words.researchSpecialInterest,(function(n){return t("li",[e._v(" "+e._s(n)+" ")])}))],2)])},Z=[];let Y=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],Y.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],Y.prototype,"dictionary",void 0),Y=Object(r["a"])([s["a"]],Y);var W=Y,B=W,Q=(n("8c41"),Object(d["a"])(B,K,Z,!1,null,"f0a999b6",null)),X=Q.exports,q=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"projects-wrapper wrapper-style href",attrs:{id:"Publication"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.conferenceTitle)+" ")])])]),t("div",e._l(e.words.conferencePublication.slice().reverse(),(function(a,o){return t("div",{key:o,staticClass:"project-component"},[t("div",{staticClass:"projects-img"},[t("a",{attrs:{href:"javascript:;"}},[t("img",{attrs:{src:n("e466")(`./${e.words.conferencePublication.length-1-o}.png`),alt:""}})])]),t("div",{staticClass:"content"},[t("strong",[e._v(e._s(a.name))]),e.words.conferencePublication.length-1-o===6?t("img",{attrs:{src:n("73d7"),alt:"new"}}):e._e(),e.words.conferencePublication.length-1-o===5?t("img",{attrs:{src:n("73d7"),alt:"new"}}):e._e(),t("p",{staticClass:"author-list",domProps:{innerHTML:e._s(a.author)}}),t("p",[e._v(e._s(a.match))]),t("p",[e._v(e._s(a.match2))]),e.words.conferencePublication.length-1-o===6?t("div",[t("a",{attrs:{href:"https://openreview.net/pdf?id=96jZFqM5E0"}},[e._v(e._s(a.paper))]),t("a",{attrs:{href:"https://openreview.net/forum?id=96jZFqM5E0"}},[e._v(e._s(a.openreview))]),t("a",{attrs:{href:""}},[e._v(e._s(a.projectPage))]),t("a",{attrs:{href:"https://github.com/ut-vision/SiMHand"}},[e._v(e._s(a.code))])]):e._e(),e.words.conferencePublication.length-1-o===5?t("div",[t("a",{attrs:{href:"https://arxiv.org/abs/2409.16816"}},[e._v(e._s(a.paper))]),t("a",{attrs:{href:""}},[e._v(e._s(a.projectPage))])]):e._e(),e.words.conferencePublication.length-1-o===4?t("div",[t("a",{attrs:{href:"https://aclanthology.org/2024.emnlp-main.797.pdf"}},[e._v(e._s(a.paper))]),t("a",{attrs:{href:"https://github.com/minglllli/CLIPFit"}},[e._v(e._s(a.code))])]):e._e(),e.words.conferencePublication.length-1-o===3?t("div",[t("a",{attrs:{href:"https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03682.pdf"}},[e._v(e._s(a.paper))]),t("a",{attrs:{href:"https://sites.google.com/view/hands2023/home"}},[e._v(e._s(a.projectPage))])]):e._e(),e.words.conferencePublication.length-1-o===2?t("div",[t("a",{attrs:{href:"https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03682.pdf"}},[e._v(e._s(a.paper))]),t("a",{attrs:{href:"https://sites.google.com/view/hands2023/home"}},[e._v(e._s(a.projectPage))])]):e._e(),e.words.conferencePublication.length-1-o===1?t("div",[t("a",{attrs:{href:"https://arxiv.org/abs/2207.05409"}},[e._v(e._s(a.paper))]),t("a",{attrs:{href:"https://github.com/dzy3/KCD"}},[e._v(e._s(a.code))])]):e._e(),e.words.conferencePublication.length-1-o===0?t("div",[t("a",{attrs:{href:"https://arxiv.org/abs/2207.03095"}},[e._v(e._s(a.paper))]),t("a",{attrs:{href:"http://www.linnie.com.cn/projects/uda_action/"}},[e._v(e._s(a.projectPage))]),t("a",{attrs:{href:"https://github.com/lin-nie/EPIC-KITCHENS-C4-UDA"}},[e._v(e._s(a.code))]),t("a",{attrs:{href:"https://www.youtube.com/watch?v=BnVhNeUBau4"}},[e._v(e._s(a.video))])]):e._e(),e._l(a.label,(function(n){return t("span",{staticClass:"label"},[e._v(e._s(n))])}))],2)])})),0)])},$=[];let ee=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],ee.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],ee.prototype,"dictionary",void 0),ee=Object(r["a"])([s["a"]],ee);var te=ee,ne=te,ae=(n("ad3d7"),Object(d["a"])(ne,q,$,!1,null,"02b963a2",null)),oe=ae.exports,ie=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"education-wrapper wrapper-style"},[t("div",[t("section",{staticClass:"title"},[t("h2",{staticClass:"href",attrs:{id:"Biography"}},[e._v(" "+e._s(e.words.educationTitle)+" ")])])]),t("div",[t("p",{staticClass:"education-content"},[e._v(" "+e._s(e.words.education.bio1.introduce)+" "),t("a",{attrs:{href:"https://cai-mj.github.io/"}},[e._v(e._s(e.words.education.bio1.mentor))]),t("br"),e._v(" "+e._s(e.words.education.bio1.brief1)),t("br"),t("strong",[e._v(e._s(e.words.education.bio1.brief2))])])])])},re=[];let se=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],se.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],se.prototype,"dictionary",void 0),se=Object(r["a"])([s["a"]],se);var ce=se,he=ce,pe=(n("f6ca"),Object(d["a"])(he,ie,re,!1,null,"7b4af97f",null)),ge=pe.exports,le=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"experience-wrapper wrapper-style href",attrs:{id:"Professional Experience"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.exchangeTitle)+" ")])])]),t("div",{staticClass:"subtitle"},[t("strong",[e._v(e._s(e.words.exchangeSubtitle))])]),t("div",{staticClass:"img-box"},e._l(e.words.exchange,(function(a){return t("div",{staticClass:"exchange-img"},[t("img",{attrs:{src:n("22e7")(`./${a.imgName}.jpg`),alt:""}}),t("a",{attrs:{href:a.href}},[t("div",{staticClass:"mask-desc"},[t("div",{staticClass:"mask-content",domProps:{innerHTML:e._s(a.intro)}})])])])})),0),t("br"),t("div",[t("ul",e._l(e.words.profExprience,(function(n){return t("li",[t("strong",[e._v(e._s(n.name))]),t("br"),e._v(" "+e._s(n.workplace)),t("span",{domProps:{innerHTML:e._s("              ")}}),e._v(e._s(n.topic)),t("br"),e._v(" "+e._s(n.title)+","+e._s(n.supervisor)+" ")])})),0)])])},ue=[];let de=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],de.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],de.prototype,"dictionary",void 0),de=Object(r["a"])([s["a"]],de);var fe=de,me=fe,we=(n("5d3f"),Object(d["a"])(me,le,ue,!1,null,"19165cb6",null)),be=we.exports,ye=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"awards-wrapper wrapper-style href",attrs:{id:"Honors"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.awardsTitle)+" ")])])]),e._l(e.words.awards,(function(n){return t("ul",{staticClass:"awards-content"},[t("p",[e._v(e._s(e.words.scholarship))]),t("a",{attrs:{href:"https://www.u-tokyo.ac.jp/en/prospective-students/fellowship.html"}},[e._v(e._s(e.words.scholar1))]),e._v("   "+e._s(e.words.scholar1explain)),t("br"),t("i",[e._v(e._s(e.words.scholar1supp))]),t("br"),t("a",{attrs:{href:"http://www.moe.gov.cn/jyb_xxgk/s5743/s5744/A05/202112/t20211216_587869.html"}},[e._v(e._s(e.words.scholar2))]),e._v("   "+e._s(e.words.scholar2explain)),t("br"),t("i",[e._v(e._s(e.words.scholar2supp))]),t("br"),e._v(" "+e._s(e.words.scholar3)),t("br"),e._v(" "+e._s(e.words.scholar4)),t("br"),e._v(" "+e._s(e.words.scholar5)),t("br"),e._v(" "+e._s(e.words.scholar6)),t("br"),e._v(" "+e._s(e.words.scholar7)),t("br"),t("p",[e._v(e._s(n.subtitle))]),e._l(n.content,(function(n){return t("li",[e._v(e._s(n))])}))],2)}))],2)},ve=[];let _e=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],_e.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],_e.prototype,"dictionary",void 0),_e=Object(r["a"])([s["a"]],_e);var Ce=_e,Se=Ce,Pe=(n("48bc"),Object(d["a"])(Se,ye,ve,!1,null,"51e36195",null)),ke=Pe.exports,je=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"publication"},[t("div",{staticClass:"patent-wrapper wrapper-style href",attrs:{id:"Patent"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.patentTitle)+" ")])])]),t("ul",{staticClass:"content"},e._l(e.words.patent,(function(n){return t("li",[t("strong",[e._v(e._s(n.name))]),t("p",{domProps:{innerHTML:e._s(n.author)}}),t("p",{domProps:{innerHTML:e._s(n.number)}})])})),0)])])},Te=[];let Ae=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],Ae.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],Ae.prototype,"dictionary",void 0),Ae=Object(r["a"])([s["a"]],Ae);var Ie=Ae,Le=Ie,xe=(n("1dd2"),Object(d["a"])(Le,je,Te,!1,null,"4b1834b5",null)),Oe=xe.exports,Ne=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"copyrght-wrapper wrapper-style href",attrs:{id:"Software Copyrght"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.copyrghtTitle)+" ")])])]),t("ul",{staticClass:"copyrght-content"},e._l(e.words.softwareCopyrght,(function(n){return t("li",[t("strong",[e._v(e._s(n.name))]),t("p",{domProps:{innerHTML:e._s(n.Number)}})])})),0)])},Ee=[];let He=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],He.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],He.prototype,"dictionary",void 0),He=Object(r["a"])([s["a"]],He);var Me=He,Re=Me,Ue=(n("3c15"),Object(d["a"])(Re,Ne,Ee,!1,null,"47db2168",null)),Fe=Ue.exports,Ve=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"tutorial-wrapper wrapper-style href",attrs:{id:"Research Funding"}},[t("div",[t("section",{staticClass:"title"},[t("h2",[e._v(" "+e._s(e.words.scientificFundTitle)+" ")])])]),t("ul",{staticClass:"content"},e._l(e.words.scientificFund,(function(n){return t("li",[t("strong",{domProps:{innerHTML:e._s(n.name)}}),t("p",{domProps:{innerHTML:e._s(n.match)}})])})),0)])},ze=[];let De=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],De.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],De.prototype,"dictionary",void 0),De=Object(r["a"])([s["a"]],De);var Ge=De,Je=Ge,Ke=(n("6b3b"),Object(d["a"])(Je,Ve,ze,!1,null,"26f15a6e",null)),Ze=Ke.exports,Ye=function(){var e=this,t=e._self._c;e._self._setupProxy;return t("div",{staticClass:"footer-wrapper wrapper-style"},[t("p",[e._v(e._s(e.words.footer.period))]),t("p",{staticClass:"update"},[t("strong",[e._v(e._s(e.words.footer.lastUpdated))])])])},We=[];let Be=class extends s["c"]{get LanguageItems(){return Object.keys(this.dictionary).map(e=>this.dictionary[e].__identity)}};Object(r["a"])([Object(p["a"])("words")],Be.prototype,"words",void 0),Object(r["a"])([Object(p["b"])("dictionary")],Be.prototype,"dictionary",void 0),Be=Object(r["a"])([s["a"]],Be);var Qe=Be,Xe=Qe,qe=(n("071d"),Object(d["a"])(Xe,Ye,We,!1,null,"c0cae402",null)),$e=qe.exports;let et=class extends s["c"]{};et=Object(r["a"])([Object(s["a"])({components:{Header:m,Profile:L,InfoList:S,NavigationBar:R,New:J,ResearchInterest:X,Publication:oe,Education:ge,ProfessionalExperience:be,Honors:ke,Patent:Oe,SoftwareCopyrght:Fe,ScientificFund:Ze,Footer:$e}})],et);var tt=et,nt=tt,at=(n("86ea"),Object(d["a"])(nt,o,i,!1,null,null,null)),ot=at.exports,it=n("2f62");const rt={__identity:"English",__langKey:"en",name:"Nie (Elon) Lin",pictureTime:"(Photo in October 2023, Tokyo, Japan)",degree:"Master Student (M2) [The University of Tokyo Fellowship Owner]",major:"Interdisciplinary Information Studies",department:"Graduate School of Interdisciplinary Information Studies (GSII)",university:"The University of Tokyo (UTokyo)",personalIntroduction:'Hi, I am a second-year CS Master Student (M2) at \n  <a href=\'https://www.u-tokyo.ac.jp/en/\' target="_blank">The University of Tokyo</a>\n  , supervised by Prof. \n  <a href=\'https://sites.google.com/ut-vision.org/ysato/\' target="_blank">Yoichi Sato</a>\n   and work as a member of <a href=\'https://www.ut-vision.org/\' target="_blank">Computer Vision Group</a>\n   at <a href=\'https://www.iis.u-tokyo.ac.jp/en/\' target="_blank">Institute of Industrial Science (IIS)</a>\n  I received my bachelor’s degree of software engineering in 2022, supervised by Prof. \n  <a href=\'https://cai-mj.github.io/\' target="_blank">Minjie Cai</a>\n  . Now I am a research intern at \n  <a href=\'https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/\' target="_blank">Microsoft Research Aisa</a>\n  , supervised by Dr. \n  <a href=\'https://recmind.cn/\' target="_blank">Dongsheng Li</a>, and work with Dr. \n  <a href=\'https://victorywys.github.io/\' target="_blank">Yansen Wang</a>\n   & Dr. \n  <a href=\'https://frosthan.github.io/\' target="_blank">Dongqi Han</a>\n   in <a href=\'https://www.microsoft.com/en-us/research/group/shanghai-ai-ml-group/\' target="_blank">Shanghai AI/ML Group</a>.\n  <br>I have previously worked with <a href="https://jp.linkedin.com/in/takeshi-ohashi-056876ba">Dr. Takeshi Ohashi</a> in the Fundamental AI Team at <a href="https://www.sony.com/en/SonyInfo/technology/about/">Sony R&D</a>.',address:"Address: Institute of Industrial Science (IIS), The University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo, 153-8505 JAPAN",email:"Email: nielin@iis.u-tokyo.ac.jp",phone:"Phone: +81 080-5637-8886 (Japan)",web:"Web: lin-nie.github.io",navigation:{name:"Nie Lin",address:["Home","News","Research Interests","Publications","Education","Professional Experience","Honors","Patents","Software Copyright","Research Funding"]},newTitle:"News",new:{new1:"[ 2019.08 ] Under the leadership of Prof. Lvy Wang, I completed a research project on mathematics and machine learning in <Strong>University of Toronto, Canada</Strong>. Lay a mathematical foundation for my future research in <Strong>Computer Vision</Strong>.",new2:"[ 2019.12 ] I won 2019 year's <Strong>The First Prize Scholarship</Strong> for being the first in my grade. Thanks!",new3:"[ 2020.03 ] During the winter vacation, I worked as a research intern under the guidance of Prof. Qing Liao from <Strong>Harbin Institute of Technology (HIT)</Strong> to complete the project of <Strong>video understanding and analysis</Strong> through deep learning.",new4:"[ 2020.08 ] I completed my exchange programme study in the field of <Strong>Artificial Intelligence and Deep Learning</Strong> in the <Strong>National University of Singapore</Strong>. And won the <Strong>Honorary Award of the National University of Singapore</Strong>.",new5:"[ 2020.10 ] I got The Third prize of <Strong>China Artificial Intelligence Electronic Design Competition</Strong>. Congratulations !!",new6:"[ 2020.12 ] I won 2020 year's <Strong>The First Prize Scholarship</Strong> and <Strong>The Kao Wei-kwong Enterprise Scholarship (Outstanding Engineering Representative) </Strong> for being the first in my grade. Thanks!",new7:"[ 2021.01 ] Our team successfully entered the <a href='https://www.ccf.org.cn/en/'>China Computer Federation (CCF) </a> <Strong>Artificial Intelligence Vision Algorithm Competition</Strong> and final with the rank of <Strong>13/2207</Strong>. A great team work experience !!",new8:"[ 2021.05 ] I won the <Strong>international second prize</Strong> in the American Mathematical Contest In Modeling (USA MCM).",new9:"[ 2021.06 ] Started research working as a <Strong>Research Assistant</Strong> at Computer Vision Lab, Hunan University. <br>Supervised by Prof. <a href='https://cai-mj.github.io/'>Minjie Cai</a>.<a href=\"https://www.linnie.com.cn/documents/Research_Assistant_Minjie_Cai_Hunan_University.pdf\">[Research Certificate]</a>",new10:"[ 2021.10 ] I got my own <Strong>head-mounted camera</Strong> from our laboratory and will be trying to collect the first-person dataset in the future. Thanks!",new11:"[ 2021.12 ] I won 2021 year's <Strong>The First Prize Scholarship</Strong> and <Strong>The Lingnan Academic Scholarship (Outstanding Academic Representative) </Strong> for being the first in my grade. Thanks!",new12:"[ 2022.05 ] Congratulations! I won the <a href='http://www.moe.gov.cn/jyb_xxgk/s5743/s5744/A05/202112/t20211216_587869.html'>National Scholarship of the People's Republic of China</a>, issued by the <a href='http://en.moe.gov.cn/'>Ministry of Education of China</a>, which is the highest level scholarship program in China! (<Strong>TOP 0.01% Students in China</Strong>).",new13:'[ 2022.06 ] My graduation thesis <Strong>"First-person Action Recognition Based on Unsupervised Domain Adaptation in Egocentric Video"</Strong> successfully pass the thesis defense of undergraduate graduation design.',new14:'[ 2022.06 ] Congratulations! My paper on <a href="https://eyewear-computing.org/EPIC_CVPR22/">CVPR-EPIC 2022</a> about <Strong>UDA Frist-person Action Recognition</Strong> has been successfully accepted, under the supervision of Prof. <a href="https://cai-mj.github.io/">Minjie Cai</a>. The arXiv and code is available. <a href="https://arxiv.org/abs/2207.03095">[ArXiv]</a> <a href="https://github.com/lin-nie/EPIC-KITCHENS-C4-UDA">[Github Code]</a>',new15:'[ 2022.07 ] I was invited to attend this year\'s <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> and participate in the <a href="https://eyewear-computing.org/EPIC_CVPR22/">EPIC 2022</a> presentation. <br><a href="./img/cvpr_2022_meeting_photo1.png">[Meeting Photo1]&<a><a href="./img/cvpr_2022_meeting_photo2.png">[Meeting Photo2]&<a><a href="./img/cvpr_2022_meeting_photo3.png">[Meeting Photo3]&<a><a href="./img/cvpr_2022_meeting_photo4.png">[Meeting Photo4]&<a><a href="./img/cvpr_2022_meeting_photo5.png">[Meeting Photo5]&<a><a href="./img/cvpr_2022_meeting_photo6.png">[Meeting Photo6]<a>',new16:'[ 2022.07 ] Our paper on <Strong>Knowledge Transfer Learning</Strong> has been accepted for <Strong>ECCV 2022</Strong>!! Paper and code is available. <br><a href="https://arxiv.org/pdf/2207.05409.pdf">[Paper]</a><a href="https://arxiv.org/abs/2207.05409">[Arxiv]</a><a href="https://github.com/dzy3/KCD">[Github Code]</a>',new17:"[ 2023.02 ] Congratulations! I successfully passed the entrance exam for the Master's program at The University of Tokyo, Graduate School of Interdisciplinary Information Studies. I will become a 2-years master student in the UTokyo this April.",new18:'[ 2023.03 ] Congratulations! I have been selected for <a href="https://www.u-tokyo.ac.jp/en/prospective-students/fellowship.html">The University of Tokyo Fellowship</a> and will receive full funding during my master\'s program in UTokyo.',new19:'[ 2023.08 ] Congratulations! I will be starting a summer AI internship with the Fundamental AI Team at <br><a href="https://www.sony.com/en/SonyInfo/technology/about/">The Sony\'s R&D</a> (Tokyo, Japan). My advisor is <a href="https://jp.linkedin.com/in/takeshi-ohashi-056876ba">Dr. Takeshi Ohashi</a>.',new20:'[ 2024.06 ] Congratulations! I will be starting a long-term research internship in <a href=\'https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/\' target="_blank">Microsoft Research Asia</a>. My advisor is <a href="https://recmind.cn/">Dr. Dongsheng Li</a> in <a href="https://www.microsoft.com/en-us/research/group/shanghai-ai-ml-group/">Shanghai AI/ML groups</a>.',new21:'[ 2024.07 ] Congratulations! Two papers were accepted by <a href="https://eccv2024.ecva.net/">ECCV 2024</a>.',new22:'[ 2024.09 ] Congratulations! One papers were accepted by <a href="https://2024.emnlp.org/">EMNLP 2024</a>.',new23:'[ 2024.12 ] Congratulations! One papers were accepted by <a href="https://2025.ieeeicassp.org/">ICASSP 2025</a>.',new24:'[ 2025.01 ] Congratulations! One papers were accepted by <a href="https://iclr.cc/">ICLR 2025</a>. Openreview is available. <a href="https://openreview.net/forum?id=96jZFqM5E0">[Openreview]</a>',new25:"[ 2025.02 ] Congratulations! I successfully passed the entrance exam for the PhD's program at <a href='https://www.u-tokyo.ac.jp/en/' target=\"_blank\">The University of Tokyo</a>, <a href='https://www.iii.u-tokyo.ac.jp/' target=\"_blank\">Graduate School of Interdisciplinary Information Studies</a>. I will become a 3-years PhD in the UTokyo this April."},educationTitle:"Education",education:{bio1:{introduce:"(2023.4 - Now) Master Student. Supervised by Prof.",mentor:"Yoichi Sato",brief1:"Interdisciplinary Information Studies, Graduate School of Interdisciplinary Information Studies (GSII)",brief2:"The University of Tokyo (UTokyo)"}},exchangeTitle:"Professional Experience",exchangeSubtitle:"I am very keen on scientific research exchange，the following are the Universities where I often communicate and study :",exchange:[{imgName:"清华大学",href:"https://www.tsinghua.edu.cn/en/",intro:'<p style="font-size: 20px">\n              Tsinghua University\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: No.17 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : No.28 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: No.20 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              ARWU 2021: No.29 in the world University Rankings\n              </p>\n               '},{imgName:"NUS",href:"https://www.nus.edu.sg/",intro:'<p style="font-size: 20px">\n              National University of Singapore\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: No.11 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : No.32 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: No.25 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n\n              </p>\n      '},{imgName:"TORONTO",href:"https://www.utoronto.ca/",intro:'<p style="font-size: 20px">\n              University of Toronto\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: No.26 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : No.17 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: No.18 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              ARWU 2021: No.23 in the world University Rankings\n              </p>\n      '},{imgName:"香港科技大学",href:"https://hkust.edu.hk/",intro:'\n              <p style="font-size: 20px">\n              Hong Kong University of Science and Technology\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: No.34 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : No.109 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: No.56 in the world University Rankings\n              </p>\n              <br><p style="font-size: 15px">\n           \n              </p>\n      '}],profExprience:[{name:"1. Hunan University (HNU)",workplace:"Changsha, China",topic:"Topic: First-person Action Recognition base on Hand Region",title:"Research Assistant",supervisor:" Supervisor: Prof. Minjie Cai [Hunan University]"},{name:"2. Harbin Institute of Technology (HIT)",workplace:"Shenzhen, China",topic:"Topic: Video Analysis and Understanding",title:"Research Intern",supervisor:" Supervisor: Prof. Qing Liao [Harbin Institute of Technology]"},{name:"3. University of Toronto (UofT)",workplace:"Toronto, Canada",topic:"Topic: Mathematics and Machine Learning",title:"Project Student",supervisor:" Supervisor: Prof. Lvy Wang [University of Toronto]"}],researchTitle:"Research Interests",overallField:"Overall Field",researchOverInterest:["Computer Vision (CV)","Machine Learning (ML)","First-person Vision (FPV)","Neuroscience (NS)"],specialField:"Special Interests",researchSpecialInterest:["1. Egocentric Video Understanding and Analysis","2. Brain-Inspired AI","3. Action Recognition","4. Human Hand Pose Analysis","4. Embodied AI",".........................."],awardsTitle:"Honors",scholarship:"Scholarship",scholar1:"1. The University of Tokyo Fellowship",scholar1explain:"[2-years, Full Funding for Master's Program]",scholar1supp:"(Only 22 Master’s course students of The University of Tokyo in 2023 year)",scholar2:"2. National Scholarship of the People's Republic of China",scholar2explain:"[Ministry of Education of China]",scholar2supp:"(TOP 0.1% Students in China)",scholar3:"3. The Lingnan Academic Scholarship  [Outstanding Academic Representative]",scholar4:"4. The First Prize Scholarship   [The First Place of GPA in the Grade, 2021]",scholar5:"5. The Kao Wei-kwong Enterprise Scholarship   [Outstanding Engineering Representative]",scholar6:"6. The First Prize Scholarship   [The First Place of GPA in the Grade, 2020]",scholar7:"7. The First Prize Scholarship   [The First Place of GPA in the Grade, 2019]",awards:[{subtitle:"Awards",content:["Outstanding Undergraduate's Thesis","International Second Prize in the American Mathematical Contest In Modeling","China Computer Federation AI Vision Algorithm Competition (Rank 13/2207)","The Third prize of China Artificial Intelligence Electronic Design Competition","Honorary Award of the National University of Singapore","Excellent Huawei Developer Award"]}],conferenceTitle:"Publications",conferencePublication:[{name:"EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022 Technical Report",author:"<Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <a href='https://cai-mj.github.io/' target=\"_blank\">Minjie Cai</a><sup>✉</sup>",match:"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR-EPIC), 2022",match2:"",paper:"[Paper]",projectPage:"[Project Page]",code:"[Github Code]",video:"[Video]"},{name:"Knowledge Condensation Distillation",author:"Chenxin Li, Mingbao Lin, Zhiyuan Ding, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, Yihong Zhuang, Yue Huang<sup>✉</sup>, Xinghao Ding, Liujuan Cao",match:"European Conference on Computer Vision (ECCV), 2022",match2:"",paper:"[Paper]",projectPage:"",code:"[Github Code]",video:""},{name:"Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects",author:"Zicong Fan, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Linlin Yang, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Zheng Liu, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, <a href='https://sites.google.com/ut-vision.org/ysato/' target=\"_blank\">Yoichi Sato</a>, Otmar Hilliges, Hyung Jin Chang, Angela Yao<sup>✉</sup>",match:"European Conference on Computer Vision (ECCV), 2024",match2:"",paper:"[Paper]",projectPage:"[Project Page]"},{name:"HandCLR: Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on Large-Scale Hand Images in the Wild",author:"<Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Mingfang Zhang, Yifei Huang, Ryosuke Furuta, <a href='https://sites.google.com/ut-vision.org/ysato/' target=\"_blank\">Yoichi Sato</a>",match:"HANDS, European Conference on Computer Vision Worshop(ECCVW), 2024",match2:"",paper:"[Paper]",projectPage:"[Project Page]"},{name:"Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",author:"Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://www.ms.k.u-tokyo.ac.jp/sugi/' target=\"_blank\">Masashi Sugiyama</a><sup>✉</sup>",match:"The Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024",match2:"",paper:"[Paper]",code:"[Github Code]"},{name:"Translating Mental Imaginations into Characters with Codebooks and Dynamics-Enhanced Decoding",author:"Jingyuan Li, <a href='https://victorywys.github.io/' target=\"_blank\">Yansen Wang</a>, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <a href='https://recmind.cn/' target=\"_blank\">Dongsheng Li</a><sup>✉</sup>",match:"IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025",match2:"",paper:"[Paper]",code:"[Github Code (Coming soon)]"},{name:"SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training",author:"<Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Mingfang Zhang, Yifei Huang, <a href='https://cai-mj.github.io/' target=\"_blank\">Minjie Cai</a>, Ming Li, Ryosuke Furuta, <a href='https://sites.google.com/ut-vision.org/ysato/' target=\"_blank\">Yoichi Sato</a><sup>✉</sup>",match:"The Thirteenth International Conference on Learning Representations (ICLR), 2025",match2:"",paper:"[Paper]",projectPage:"[Project Page (Coming soon)]",code:"[Github Code]",openreview:"[Openreview]"}],journalTitle:"Journal Publication",journalPublication:[{}],patentTitle:"Patents",patent:[{name:"Human-computer Interaction (HCI) Sensing Devices based on Analog Signal Processing",author:'<a href="">Nie Lin</a>',number:"CN 202046227225.2"}],copyrghtTitle:"Software Copyright",softwareCopyrght:[{name:"OCR Recognition System for Japanese Postal Payment Notes",Number:"No.A0003976 in SoftwareCopyright"},{name:"Video copyright protection system based on artificial intelligence",Number:"No.4840268 in SoftwareCopyright"}],projectsTitle:"Projects",projectsHightLight:"Highlighted",projectsNote:" projects are recently projects.",recentlyProjects:[{name:"EgoV: A New Datasets of Egocentric Videos Across Real and Virtual",author:"<strong>!! As a new long-term research project will be carried out at my master's level !!</strong>",match:"",match2:"",paper:"[Research Proposal]",projectPage:"[Project Page]",code:"[Github Code]",video:"[Video]"},{name:"UDA First-person Action Recognition based on Hand Regions in Egocentric Videos",author:"<strong>Nie Lin</strong>, Minjie Cai",match:"Hunan University",match2:"",paper:"[Paper]",projectPage:"[Project Page]",code:"[Github Code]",video:"[Video]",photo:"[Meeting Photo]"},{name:"Knowledge Condensation Distillation base on Transfer Learning",author:"Chenxin Li, Mingbao Lin, <strong>Nie Lin</strong>, Yihong Zhuang, Yue Huang",match:"Xiamen & Hunan University, Tencent Youtu Lab",paper:"[Paper]",projectPage:"[Project Page]",code:"[Github Code]",video:"[Video]"},{name:"Dataset Acquisition from First-person Perspective (Through Head-mounted Camera)",author:"<strong>Nie Lin</strong>",match:"Hunan University",paper:"",projectPage:"",code:"",video:""}],pastProjects:[{name:"Multi-Modal Video Analysis and Understanding",author:"<strong>Nie Lin</strong>, Fan Guo, Jie Wang, Ye Ding, Qing Liao",match:"Harbin Institute of Technology (HIT) ",match2:"",paper:"",projectPage:"",code:"",video:"[video]"},{name:"Hand-ball Action Recognition Control System based on OpenMV Machine Vision",author:"<strong>Nie Lin</strong>, Bing Wang, Qingfeng Zhou",match:"China Artificial Intelligence Electronic Design Competition",match2:"",paper:"",projectPage:"",code:"",video:""}],scientificFundTitle:"Research Funding",scientificFund:[{name:'<a href="https://projectdb.jst.go.jp/grant/JST-PROJECT-23837230/">Establishing an International Collaborative Research Network on Human-Centered Vision and Media Technologies</a>',match:"JST ASPIRE Grant Number JPMJAP2303"},{name:'<a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-24K02956/">Semantic Deepening AI of Video-based Human Behavior Understanding</a>',match:"JSPS KAKENHI Grant Number JP24K02956,"},{name:'<a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-22KF0119/">Learning and Prediction of Human Behavior via Multimodal Analysis of First-Person Perspective Instructional Videos.</a>',match:"JSPS KAKENHI Grant Number JP22KF0119,"},{name:'<a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">Human Behavior Understanding via Imitative AI</a>',match:"JST ACT-X Grant Number JPMJAX2007"}],footer:{period:"© 2018 - 2025   Nie Lin",lastUpdated:"Last updated: February 13,2025"}};var st=rt;const ct={__identity:"日本語",__langKey:"jp",name:"林 涅（リン ネ）",pictureTime:"(撮影日：2023年10月、日本、東京)",degree:"修士 (M2) [東京大学フェローシップ - オーナー]",major:"学際情報学",department:"学際情報学府",university:"東京大学",personalIntroduction:'こんにちは、私は\n  <a href=\'https://www.u-tokyo.ac.jp/ja/index.html\' target="_blank">東京大学</a>\n  の学際情報学専攻の修士2年生です。\n  <a href=\'https://sites.google.com/ut-vision.org/ysato/\' target="_blank">佐藤洋一</a>\n  教授の指導の下、\n  <a href=\'https://www.iis.u-tokyo.ac.jp/ja/\' target="_blank">生産技術研究所 (IIS)</a>\n  の\n  <a href=\'https://www.ut-vision.org/ja/\' target="_blank">コンピュータビジョングループ</a>\n  の一員として研究を行っています。2022年にソフトウェア工学の学士号を取得し、その間、\n  <a href=\'https://cai-mj.github.io/\' target="_blank">蔡敏捷</a>\n  教授の指導を受けました。現在、私は\n  <a href=\'https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/\' target="_blank">マイクロソフトアジア研究院</a>\n  でインターンシップを行っており、\n  <a href=\'https://recmind.cn/\' target="_blank">李東勝</a>\n  博士の指導の下、\n  <a href=\'https://victorywys.github.io/\' target="_blank">王延森</a>\n  博士と\n  <a href=\'https://frosthan.github.io/\' target="_blank">韓東起</a>\n  博士と共に\n  <a href=\'https://www.microsoft.com/en-us/research/group/shanghai-ai-ml-group/\' target="_blank">Shanghai AI/ML Group</a>で研究しています。\n <span style="display:inline-block; width: 40ch; height: 1em; background-color: transparent;"></span>\n私は以前、<a href="https://jp.linkedin.com/in/takeshi-ohashi-056876ba">大橋岳史</a>博士と共に、<a href="https://www.sony.com/en/SonyInfo/technology/about/">ソニーR&D</a>の基盤AIチームで働いていました。',address:"アドレス: 〒 153-8505 東京都目黒区駒場 4-6-1 東京大学 生産技術研究所",email:"メールアドレス: nielin@iis.u-tokyo.ac.jp",phone:"電話番号: +81 080-5637-8886 (日本)",web:"ホームページ: lin-nie.github.io",newTitle:"ニュース",navigation:{name:"リン ネエ",address:["ホームページ","ニュース","研究趣味","発表論文","学歴","専門経験","奨学金・受賞","発明特许","ソフトウエア著作権","研究ファンド"]},new:{new1:"[ 2019.08 ] Prof. Lvy Wangのご指導により、<strong>カナダのトロント大学（UofT）</strong>で私は数学と機械学習に関する研究プロジェクトを取り組みました。今後の<strong>コンピュータビジョン</strong>に関する研究に数学の基礎を築きくれました。",new2:"[ 2019.12 ] 私は<strong>学年トップ1の成績</strong>で2019年度の<strong>一等奨学金</strong>を受賞しました。ありがとうございます!",new3:"[ 2020.03 ] 二年生の冬休み、<strong>ハルビン工業大学（HIT）</strong>の<strong>Prof. Liao</strong>のご指導により、私は研究実習に参加し、ディープ・ラーニングを用いて<strong>ビデオの理解と分析</strong>に関するプロジェクトを完成しました。",new4:"[ 2020.08 ] 私は<strong>シンガポール国立大学（NUS）</strong>で<strong>人工知能とディープラーニングのプロジェクトスタディ</strong>を修了しました。シンガポール国立大学から<strong>栄誉賞</strong>を受賞しました。",new5:"[ 2020.10 ] <Strong>中国人工知能電子デザインコンテスト</Strong>で三等賞を受賞しました !!",new6:"[ 2020.12 ] 私は<strong>学年トップ1の成績</strong>で2020年の<strong>一等奨学金</Strong>及び<Strong>高偉光企業奨学金(傑出工程代表)</strong>を受賞しました。ありがとうございます。",new7:'[ 2021.01 ] うちのチームは13/2207の順位で<a href= "https://www.ccf.org.cn/en/">中国コンピュータ学会（CCF）</a><Strong>人工知能視覚アルゴリズムコンテストの決勝戦</Strong>に入選しました。素晴らしいチームワークでした！！',new8:"[ 2021.05 ] 【Congrats！】アメリカ数学モデリングコンテスト(USA MCM/ICM)で<Strong>国際二等賞</Strong>を受賞しました！",new9:'[ 2021.06 ] 湖南大学のコンピュータビジョン研究室で<strong>アシスタント研究員</strong>として務め始めました。<br>指導先生は<a href="https://cai-mj.github.io/">蔡 敏捷教授</a>です, よろしくお願いします~<a href="https://www.linnie.com.cn/documents/Research_Assistant_Minjie_Cai_Hunan_University.pdf">[研究證明]</a>',new10:"[ 2021.10 ] 研究室から自分の<Strong>ヘッドセット・カメラ</Strong>をゲットしました。これを使って<Strong>一人称データセット</Strong>の収集を試みます。ありがとうございます。",new11:"[ 2021.12 ] 私は<Strong>学年トップ1の成績</Strong>で2021年の<Strong>一等奨学金</Strong>及び<Strong>嶺南学術奨学金(優秀学術代表)</Strong>を獲得しました。ありがとうございます。",new12:"[ 2022.05 ] 【Congrats！】<a href='http://en.moe.gov.cn/'>中華人民共和国教育部</a>から<a href='http://www.moe.gov.cn/jyb_xxgk/s5743/s5744/A05/202112/t20211216_587869.html'>国家レベル奨学金</a>を受賞しました。これは中国最高レベルの奨学金プログラムです。（<Strong>全国上位0.01%の学生</Strong>）.",new13:"[ 2022.06 ] 私の卒業論文<strong>「エゴセントリックビデオにおけるUDA適応に基づく一人称行動認識」</strong>は、無事に答弁に合格しました。",new14:'[ 2022.06 ] 【Congrats！】<a href="https://cai-mj.github.io/">蔡敏捷 先生</a>のご指導のもとで、私の<Strong>無監督ドメイン適応について一人称動作認識</Strong>に関する論文は<a href="https://eyewear-computing.org/EPIC_CVPR22/">CVPR-EPIC 2022</a>に採択されました。論文とコードは利用可能です。<a href="https://arxiv.org/abs/2207.03095">[ArXiv 预印本]</a> <a href="https://github.com/lin-nie/EPIC-KITCHENS-C4-UDA">[Github代码]</a>',new15:'[ 2022.07 ] 私は今年の<a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>に招待されて、<a href="https://eyewear-computing.org/EPIC_CVPR22/">EPIC 2022</a>の講演に参加しました。<br><a href="./img/cvpr_2022_meeting_photo1.png">[会議の写真1]&<a><a href="./img/cvpr_2022_meeting_photo2.png">[会議の写真2]&<a><a href="./img/cvpr_2022_meeting_photo3.png">[会議の写真3]&<a><a href="./img/cvpr_2022_meeting_photo4.png">[会議の写真4]&<a><a href="./img/cvpr_2022_meeting_photo5.png">[会議の写真5]&<a><a href="./img/cvpr_2022_meeting_photo6.png">[会議の写真6]<a>.',new16:'[ 2022.07 ] 私たちのノウレッジ・トランスファー・ラーニングに関する論文はECCV 2022に採択されました。論文とコードは利用可能です。<a href="https://arxiv.org/pdf/2207.05409.pdf">[論文]</a><a href="https://arxiv.org/abs/2207.05409">[Arxiv プリプリント]</a><a href="https://github.com/dzy3/KCD">[Github コード]</a>',new17:"[ 2023.02 ] 【Congrats！】私は東京大学 情報学環・学際情報学府（GSII）の修士課程入学試験に合格し、4月から東京大学学際情報学府の学際情報学専攻の修士になります。",new18:'[ 2023.03 ] 【Congrats！】私は <a href="https://www.u-tokyo.ac.jp/en/prospective-students/fellowship.html">東京大学フェローシップ</a> に選ばれ、東京大学での修士課程において全額奨学金を受けることになります。',new19:'[ 2023.08 ] 【Congrats！】私は <a href="https://www.sony.com/en/SonyInfo/technology/about/">ソニーR&D</a>（日本、東京）の基盤AIチームで産学連携インターンシップを開始する予定です。私の指導教員は <a href="https://jp.linkedin.com/in/takeshi-ohashi-056876ba">大橋健史博士</a> です。',new20:'[ 2024.06 ] 【Congrats！】私は <a href=\'https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/\' target="_blank">Microsoft Research Asia</a> で産学連携インターンシップを開始する予定です。私の指導教員は <a href="https://recmind.cn/">李東升博士</a> で、彼は <a href="https://www.microsoft.com/en-us/research/group/shanghai-ai-ml-group/">Shanghai AI/ML Group</a> に所属しています。',new21:'[ 2024.07 ] 【Congrats！】2本の論文が <a href="https://eccv2024.ecva.net/">ECCV 2024</a> に採択されました。',new22:'[ 2024.09 ] 【Congrats！】1本の論文が <a href="https://2024.emnlp.org/">EMNLP 2024</a> に採択されました。',new23:'[ 2024.12 ] 【Congrats！】1本の論文が <a href="https://2025.ieeeicassp.org/">ICASSP 2025</a> に採択されました。',new24:'[ 2025.01 ] 【Congrats！】1本の論文が <a href="https://iclr.cc/">ICLR 2025</a> に採択され、Openreviewが公開されました。 <a href="https://openreview.net/forum?id=96jZFqM5E0">[Openreview]</a>',new25:"[ 2025.02 ] 【Congrats！】私は <a href='https://www.u-tokyo.ac.jp/en/' target=\"_blank\">東京大学</a> の博士課程入試に合格し、2025年4月から <a href='https://www.iii.u-tokyo.ac.jp/' target=\"_blank\">東京大学大学院情報学環</a> で3年間の博士課程を開始します。"},educationTitle:"学歴",education:{bio1:{introduce:"(2023.4-至今)  修士. 指導教員は教授",mentor:"佐藤 洋一",brief1:"学際情報学, 学際情報学府",brief2:"東京大学"}},exchangeTitle:"専門経験",exchangeSubtitle:"私は非常に科学研究の交流に熱心です、以下はよく交流して勉強している大学です：",exchange:[{imgName:"清华大学",href:"https://www.tsinghua.edu.cn/en/",intro:'<p style="font-size: 20px">\n              清華大学\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: 世界大学のランキングーーNo.17\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : 世界大学のランキングーーNo.28\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: 世界大学のランキングーーNo.20\n              </p>\n              <br><p style="font-size: 15px">\n              ARWU 2021: 世界大学のランキングーーNo.29\n              </p>\n                '},{imgName:"NUS",href:"https://www.nus.edu.sg/",intro:'<p style="font-size: 20px">\n              シンガポール国立大学（NUS）\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: 世界大学のランキングーーNo.11\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : 世界大学のランキングーーNo.32\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: 世界大学のランキングーーNo.25\n              </p>\n              <br><p style="font-size: 15px">\n\n              </p>\n      '},{imgName:"TORONTO",href:"https://www.utoronto.ca/",intro:'<p style="font-size: 20px">\n              トロント大学（UofT）\n              </p> \n              <br><p style="font-size: 14.9px">\n              QS 2021: 世界大学のランキングーーNo.26\n              </p>\n              <br><p style="font-size: 14.9px">\n              U.S. New : 世界大学のランキングーーNo.17\n              </p>\n              <br><p style="font-size: 14.9px">\n              THE 2021: 世界大学のランキングーーNo.18\n              </p>\n              <br><p style="font-size: 14.9px">\n              ARWU 2021: 世界大学のランキングーーNo.23\n              </p>\n      '},{imgName:"香港科技大学",href:"https://hkust.edu.hk/",intro:'\n              <p style="font-size: 20px">\n              ホンコン科技大学（HKUST）\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: 世界大学のランキングーーNo.34\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : 世界大学のランキングーーNo.109\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: 世界大学のランキングーーNo.56\n              </p>\n              <br><p style="font-size: 15px">\n           \n              </p>\n      '}],profExprience:[{name:"1. 湖南大学 (HNU)",workplace:"長沙, 中国",topic:"研究テーマ: 手部領域に基づく一人称働作認識",title:"アシスタント研究員",supervisor:" 指導教授: 蔡 敏捷 教授 [湖南大学]"},{name:"2. 哈尔滨工业大学 (HIT)",workplace:"深圳, 中国",topic:"研究课题: 视频分析与理解",title:"研究实习生",supervisor:" 指导教授: 廖清教授 [哈尔滨工业大学(深圳)]"},{name:"3. 多伦多大学 (UofT)",workplace:"多伦多, 加拿大",topic:"研究课题: 数学与机器学习",title:"项目学生",supervisor:" 指导教授: Lvy Wang教授 [多伦多大学]"}],researchTitle:"研究趣味",overallField:"Overall Field",researchOverInterest:["コンピュータビジョン (CV)","機械学習 (ML)","第一人称視点 (PV)","神経科学 (NS)"],specialField:"Special Interests",researchSpecialInterest:["1. 自己中心的なビデオ理解と分析","2. 脳インスパイアード AI","3. 動作認識","4. 人間の手のポーズ分析","5. 具身知能 AI",".........................."],awardsTitle:"奨学金・受賞",scholarship:"奨学金",scholar1:"1. 東京大学フェローシップ",scholar1explain:"[2年間、修士課程の全額奨学金]",scholar1supp:"(2023年、東京大学の修士課程学生は22名のみ)",scholar2:"2. 中華人民共和国国家奨学金",scholar2explain:"[中華人民共和国教育部]",scholar2supp:"(中国の上位0.1%の学生)",scholar3:"3. 嶺南学術奨学金  [傑出した学術の代表]",scholar4:"4. 2021年度 一等奨学金   [学年GPA 1位、2021年]",scholar5:"5. 高偉光企業奨学金   [傑出した工事の代表]",scholar6:"6. 2020年度 一等奨学金   [学年GPA 1位、2020年]",scholar7:"7. 2019度 一等奨学金   [学年GPA 1位、2019年]",awards:[{subtitle:"受賞",content:["優秀な学部生の論文","米国数学モデリングコンテスト国際2位","中国コンピュータ学会AI視覚アルゴリズム大会 (順位13/2207)","中国人工知能電子デザインコンテスト3等賞","シンガポール国立大学名誉賞","ファーウェイ優秀開発者賞"]}],conferenceTitle:"発表論文",conferencePublication:[{name:"EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022 Technical Report",author:"<Strong>Nie Lin</Strong>, <a href='https://cai-mj.github.io/' target=\"_blank\">Minjie Cai</a><sup>✉</sup>",match:"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR-EPIC), 2022",match2:"",paper:"[論文]",projectPage:"[プロジェクト ページ]",code:"[コード]",video:"[ビデオ]"},{name:"Knowledge Condensation Distillation",author:"Chenxin Li, <a href='https://lmbxmu.github.io/'>Mingbao Lin</a>, Zhiyuan Ding, <Strong>Nie Lin</Strong>, Yihong Zhuang, <a href='https://huangyue05.github.io/'>Yue Huang</a>*,...",match:"European Conference on Computer Vision (ECCV), 2022",match2:"",paper:"[論文]",projectPage:"",code:"[コード]",video:""},{name:"Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects",author:"Zicong Fan, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Linlin Yang, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Zheng Liu, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao",match:"European Conference on Computer Vision (ECCV), 2024",match2:"",paper:"[論文]",projectPage:"[プロジェクト ページ]"},{name:"HandCLR: Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on Large-Scale Hand Images in the Wild",author:"<Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Mingfang Zhang, Yifei Huang, Ryosuke Furuta, <a href='https://sites.google.com/ut-vision.org/ysato/' target=\"_blank\">Yoichi Sato</a>",match:"HANDS, European Conference on Computer Vision Worshop(ECCVW), 2024",match2:"",paper:"[論文]",projectPage:"[プロジェクト ページ]"},{name:"Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",author:"Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://www.ms.k.u-tokyo.ac.jp/sugi/' target=\"_blank\">Masashi Sugiyama</a><sup>✉</sup>",match:"The Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024",match2:"",paper:"[論文]",code:"[コード]"},{name:"Translating Mental Imaginations into Characters with Codebooks and Dynamics-Enhanced Decoding",author:"Jingyuan Li, <a href='https://victorywys.github.io/' target=\"_blank\">Yansen Wang</a>, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <a href='https://recmind.cn/' target=\"_blank\">Dongsheng Li</a><sup>✉</sup>",match:"IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025",match2:"",paper:"[論文]",code:"[コード (Coming soon)]"},{name:"SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training",author:"<Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Mingfang Zhang, Yifei Huang, <a href='https://cai-mj.github.io/' target=\"_blank\">Minjie Cai</a>, Ming Li, Ryosuke Furuta, <a href='https://sites.google.com/ut-vision.org/ysato/' target=\"_blank\">Yoichi Sato</a><sup>✉</sup>",match:"The Thirteenth International Conference on Learning Representations (ICLR), 2025",match2:"",paper:"[論文]",projectPage:"[プロジェクト ページ (Coming soon)]",code:"[コード]",openreview:"[Openreview]"}],journalTitle:"ジャーナル論文",journalPublication:[{name:"暂無"}],patentTitle:"発明特许",patent:[{name:"アナログ信号処理ベースのヒューマンインタラクション感知装置",author:'<a href="#">林 涅</a>',number:"CN 202046227225.2"}],copyrghtTitle:"ソフトウエア著作権",softwareCopyrght:[{name:"日本郵便の支払伝票OCR認識システム",Number:"ソフトウエア著作権 No.A0003976"},{name:"人工知能による映像著作権保護システム",Number:"ソフトウエア著作権 No.4840268"}],projectsTitle:"プロジェクト",projectsHightLight:"Highlighted",projectsNote:" は最近取り組んだプロジェクトについて言及した。",recentlyProjects:[{name:"EgoV: リアルとバーチャルの自己中心的なビデオの全く新しいデータセットです",author:"<strong>!! 新しい長期プロジェクトとして修士課程で始めます !!</strong>",match:"",match2:"",paper:"[研究計画書]",projectPage:"[プロジェクト ページ]",code:"[コード]",video:"[ビデオ]"},{name:"自己中心ビデオの手領域に基づく監督のない領域適応一人称動作認識",author:"<strong>林 涅</strong>, 蔡 敏捷",match:"湖南大学",match2:"",paper:"[論文]",projectPage:"[プロジェクト ページ]",code:"[コード]",video:"[ビデオ]",photo:"[会議の写真]"},{name:"移動学習による知識濃縮蒸留",author:"李 宸鑫, 林 明寶, <strong>林 涅</strong>, 莊 毅鴻, 黃 悅",match:"廈門大学 & 湖南大学",paper:"[論文]",projectPage:"[プロジェクト ページ]",code:"[コード]",video:"[ビデオ]"},{name:"一人称視点でのデータ収集 (ヘッドマウントカメラによる)",author:"<strong>林 涅</strong>",match:"湖南大学",paper:"",projectPage:"",code:"",video:""}],pastProjects:[{name:"マルチモダリティ ビデオ分析と理解",author:"<strong>林 涅</strong>, 郭 凡, 王 傑, 丁 燁, 廖 清",match:"ハルビン工業大学 (HIT) ",match2:"",paper:"",projectPage:"",code:"",video:""},{name:"OpenMVマシンビジョンによる ハン-ドボール 動作認識制御システム",author:"<strong>林 涅</strong>, 王 斌, 周 清峰",match:"中国人工知能電子デザインコンテスト",match2:"",paper:"",projectPage:"",code:"",video:""}],scientificFundTitle:"研究ファンド",scientificFund:[{name:'<a href="https://projectdb.jst.go.jp/grant/JST-PROJECT-23837230/">人間中心のビジョン・メディア技術に関する国際共同研究ネットワークの構築</a>',match:"JST ASPIRE Grant Number JPMJAP2303"},{name:'<a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-24K02956/">映像に基づく人物行動理解の意味的AI深化</a>',match:"JSPS KAKENHI Grant Number JP24K02956,"},{name:'<a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-22KF0119/">一人称視点教示映像のマルチモーダル解析による人物行動の学習と予測</a>',match:"JSPS KAKENHI Grant Number JP22KF0119,"},{name:'<a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">模倣型AIによる人間行動の理解</a>',match:"JST ACT-X Grant Number JPMJAX2007"}],footer:{period:"© 2018 - 2022   林涅",lastUpdated:"前回のアップデート: 2025年02月13日"}};var ht=ct;const pt={__identity:"中文",__langKey:"zh",name:"林 涅",pictureTime:"(拍摄于2023年10月，日本，东京)",degree:"硕士 (M2) [东京大学奖学金 支助]",major:"交叉信息学",department:"跨学科交叉信息学院",university:"东京大学",personalIntroduction:'你好，我是\n  <a href=\'https://www.u-tokyo.ac.jp/zh/index.html\' target="_blank">东京大学</a>\n  跨学科交叉信息学硕士二年级的研究生，在\n  <a href=\'https://sites.google.com/ut-vision.org/ysato/\' target="_blank">佐藤洋一</a>\n  教授的指导下，作为東京大学\n  <a href=\'https://www.iis.u-tokyo.ac.jp/en/\' target="_blank">生産技術研究所 (IIS)</a>\n  —\n  <a href=\'https://www.ut-vision.org/\' target="_blank">计算机视觉小组</a>\n  的成员进行研究。我于2022年获得软件工程学士学位，在此期间接受\n  <a href=\'https://cai-mj.github.io/\' target="_blank">蔡敏捷</a>\n  教授的指导。目前，我在\n  <a href=\'https://www.microsoft.com/en-us/research/lab/microsoft-research-asia-zh-cn/\' target="_blank">微软亚洲研究院</a>\n  实习，指导导师是\n  <a href=\'https://recmind.cn/\' target="_blank">李东胜</a>\n  博士，并与\n  <a href=\'https://victorywys.github.io/\' target="_blank">王延森</a>\n  博士和\n  <a href=\'https://frosthan.github.io/\' target="_blank">韩东起</a>\n  博士一起在 \n  <a href=\'https://www.microsoft.com/en-us/research/group/shanghai-ai-ml-group/?locale=zh-cn\' target="_blank">Shanghai AI/ML Group</a> 进行研究。\n  我曾经在索尼R&D的基础AI团队与<a href="https://jp.linkedin.com/in/takeshi-ohashi-056876ba">大桥武史</a>博士共同工作过，团队隶属于 <a href="https://www.sony.com/en/SonyInfo/technology/about/">索尼研发部门</a>。',address:"地址: 〒 153-8505 東京都目黒区駒場 4-6-1 東京大学 生産技術研究所",email:"个人邮箱: nielin@iis.u-tokyo.ac.jp",phone:"个人电话: +81 080-5637-8886 (日本)",web:"个人网站: lin-nie.github.io",newTitle:"消息",navigation:{name:"林涅",address:["主页","消息","研究兴趣","论文发表","教育","专业经验","荣誉奖项","发表专利","软件著作","参与基金"]},new:{new1:"[ 2019.08 ] 在Prof. Lvy Wang的指导下，我在<strong>加拿大多伦多大学</strong>完成了一个关于数学与机器学习的研究项目。为我以后关于<strong>计算机视觉</strong>的研究奠定数学基础。",new2:"[ 2019.12 ] 我以年级第一的优异成绩获得了2019年的<strong>一等奖学金</strong>。谢谢!",new3:"[ 2020.03 ] 在大二寒假期间，我在<strong>哈尔滨工业大学（深圳）</strong>廖教授的指导下参与研究实习，通过深度学习完成关于<strong>视频理解与分析</strong>项目。",new4:"[ 2020.08 ] 我在<strong>新加坡国立大学</strong>完成了<strong>人工智能与深度学习</strong>领域的项目学习。并获得由新加坡国立大学颁发的<strong>荣誉奖</strong>。",new5:"[ 2020.10 ] 我获得中国人工智能电子设计大赛三等奖 !!",new6:"[ 2020.12 ] 我以年级第一的优异成绩获得了2020年的<Strong>一等奖学金</Strong>以及<Strong> 高伟光企业奖学金(杰出工程代表) </Strong>。谢谢!",new7:'[ 2021.01 ] 我们团队以<Strong>13/2207</Strong>的排名进入<a href="https://www.ccf.org.cn/en/">中国计算机学会(CCF) </a> <Strong>人工智能视觉算法大赛</Strong>决赛。一次很赞的团队合作体验!!',new8:"[ 2021.05 ] 我在美国数学建模竞赛USA MCM/ICM 中获得<Strong>国际二等奖</Strong>。",new9:"[ 2021.06 ] 开始在湖南大学的計算機視覺实验室担任 <Strong>研究助理</Strong>。 由<a href='https://cai-mj.github.io/'>蔡 敏捷</a>教授担任指导老师。<a href=\"https://www.linnie.com.cn/documents/Research_Assistant_Minjie_Cai_Hunan_University.pdf\">[研究證明]</a>",new10:"[ 2021.10 ] 我从我们的实验室得到了属于我自己的<Strong>头戴式相机</Strong>，未来将尝试收集第一人称数据集。谢谢!",new11:"[ 2021.12 ] 我以年级第一的优异成绩获得了2021年的<Strong>一等奖奖学金</Strong>、<Strong>岭南学术奖学金(优秀学术代表)</Strong>，谢谢!",new12:"[ 2022.05 ] 恭喜! 我获得了<a href='http://www.moe.gov.cn/jyb_xxgk/s5743/s5744/A05/202112/t20211216_587869.html'>中华人民共和国国家奖学金</a>, 由<a href='http://en.moe.gov.cn/'>中华人民共和国教育部</a>颁布, 这是中国最高级别的奖学金项目! (<Strong>全国排名前0.01%的学生</Strong>).",new13:"[ 2022.06 ] 我的毕业论文<strong>《基于自我中心视频中无监督域适应的第一人称动作识别》</strong>顺利通过了本科毕业设计论文答辩。",new14:'[ 2022.06 ] 恭喜! 我的论文在<a href="https://eyewear-computing.org/EPIC_CVPR22/">CVPR-EPIC 2022</a>中关于<Strong>无监督域适应第一人称动作识别</Strong>顺利被接收, 在<a href="https://cai-mj.github.io/">蔡 敏捷</a>教授的指导下。 预印本和代码均可用. <a href="https://arxiv.org/abs/2207.03095">[預印本]</a> <a href="https://github.com/lin-nie/EPIC-KITCHENS-C4-UDA">[Github代码]</a>',new15:'[ 2022.07 ] 我受邀参加今年<a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> 并参与 <a href="https://eyewear-computing.org/EPIC_CVPR22/">EPIC 2022</a> 演讲。<br><a href="./img/cvpr_2022_meeting_photo1.png">[會議照片1]&<a><a href="./img/cvpr_2022_meeting_photo2.png">[會議照片2]&<a><a href="./img/cvpr_2022_meeting_photo3.png">[會議照片3]&<a><a href="./img/cvpr_2022_meeting_photo4.png">[會議照片4]&<a><a href="./img/cvpr_2022_meeting_photo5.png">[會議照片5]&<a><a href="./img/cvpr_2022_meeting_photo6.png">[會議照片6]<a>',new16:'[ 2022.07 ] 我们关于<Strong>知识迁移学习</Strong>的论文已经被今年的<Strong>ECCV 2022</Strong>正式接受!! 代码已经开源。<a href="https://arxiv.org/pdf/2207.05409.pdf">[論文]</a><a href="https://arxiv.org/abs/2207.05409">[預印本]</a><a href="https://github.com/dzy3/KCD">[Github代碼]</a>',new17:"[ 2023.02 ] 恭喜！我成功通过了 <a href='https://www.u-tokyo.ac.jp/en/' target=\"_blank\">东京大学</a> 的硕士入学考试，将于今年四月开始在 <a href='https://www.iii.u-tokyo.ac.jp/' target=\"_blank\">情报学环·跨学科交叉信息学院（GSII）</a>攻读为期两年的硕士学位。",new18:'[ 2023.03 ] 恭喜！我已被 <a href="https://www.u-tokyo.ac.jp/en/prospective-students/fellowship.html">东京大学奖学金</a> 选中，并将在东京大学的硕士项目中获得全额资助。',new19:'[ 2023.08 ] 恭喜！我将开始在 <br><a href="https://www.sony.com/en/SonyInfo/technology/about/">索尼公司研发部门</a>（日本，东京）的基础人工智能团队进行暑期AI实习。我的指导老师是 <a href="https://jp.linkedin.com/in/takeshi-ohashi-056876ba">大桥健史博士</a>。',new20:'[ 2024.06 ] 恭喜！我将开始在 <a href=\'https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/\' target="_blank">微软亚洲研究院</a> 进行长期研究实习。我的指导老师是 <a href="https://recmind.cn/">李东升博士</a>，他属于 <a href="https://www.microsoft.com/en-us/research/group/shanghai-ai-ml-group/">上海AI/ML团队</a>。',new21:'[ 2024.07 ] 恭喜！两篇论文被 <a href="https://eccv2024.ecva.net/">ECCV 2024</a> 接收。',new22:'[ 2024.09 ] 恭喜！一篇论文被 <a href="https://2024.emnlp.org/">EMNLP 2024</a> 接收。',new23:'[ 2024.12 ] 恭喜！一篇论文被 <a href="https://2025.ieeeicassp.org/">ICASSP 2025</a> 接收。',new24:'[ 2025.01 ] 恭喜！一篇论文被 <a href="https://iclr.cc/">ICLR 2025</a> 接收，Openreview已上线。 <a href="https://openreview.net/forum?id=96jZFqM5E0">[Openreview]</a>',new25:"[ 2025.02 ] 恭喜！我成功通过了 <a href='https://www.u-tokyo.ac.jp/en/' target=\"_blank\">东京大学</a> 的博士入学考试，将于今年四月开始在 <a href='https://www.iii.u-tokyo.ac.jp/' target=\"_blank\">情报学环·跨学科交叉信息学院（GSII）</a>攻读为期三年的博士学位。"},educationTitle:"教育",education:{bio1:{introduce:"(2023.4-至今)  硕士. 指导教授为",mentor:"佐藤 洋一",brief1:"交叉信息学, 跨学科交叉信息学院",brief2:"东京大学"}},exchangeTitle:"专业经验",exchangeSubtitle:"本人非常热衷于科研交流，以下为经常交流学习的大学：",exchange:[{imgName:"清华大学",href:"https://www.tsinghua.edu.cn/en/",intro:'<p style="font-size: 20px">\n              清華大学\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: 世界大学排名ーーNo.17\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : 世界大学排名ーーNo.28\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: 世界大学排名ーーNo.20\n              </p>\n              <br><p style="font-size: 15px">\n              ARWU 2021: 世界大学排名ーーNo.29\n              </p>\n                '},{imgName:"NUS",href:"https://www.nus.edu.sg/",intro:'<p style="font-size: 20px">\n              シンガポール国立大学（NUS）\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: 世界大学排名ーーNo.11\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : 世界大学排名ーーNo.32\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: 世界大学排名ーーNo.25\n              </p>\n              <br><p style="font-size: 15px">\n\n              </p>\n      '},{imgName:"TORONTO",href:"https://www.utoronto.ca/",intro:'<p style="font-size: 20px">\n              トロント大学（UofT）\n              </p> \n              <br><p style="font-size: 14.9px">\n              QS 2021: 世界大学排名ーーNo.26\n              </p>\n              <br><p style="font-size: 14.9px">\n              U.S. New : 世界大学排名ーーNo.17\n              </p>\n              <br><p style="font-size: 14.9px">\n              THE 2021: 世界大学排名ーーNo.18\n              </p>\n              <br><p style="font-size: 14.9px">\n              ARWU 2021: 世界大学排名ーーNo.23\n              </p>\n      '},{imgName:"香港科技大学",href:"https://hkust.edu.hk/",intro:'\n              <p style="font-size: 20px">\n              ホンコン科技大学（HKUST）\n              </p> \n              <br><p style="font-size: 15px">\n              QS 2021: 世界大学排名ーーNo.34\n              </p>\n              <br><p style="font-size: 15px">\n              U.S. New : 世界大学排名ーーNo.109\n              </p>\n              <br><p style="font-size: 15px">\n              THE 2021: 世界大学排名ーーNo.56\n              </p>\n              <br><p style="font-size: 15px">\n           \n              </p>\n      '}],profExprience:[{name:"1. 湖南大学 (HNU)",workplace:"长沙, 中国",topic:"研究课题: 基于手部区域的第一人称动作识别",title:"助理研究员",supervisor:" 指导教授: 蔡 敏捷 教授 [湖南大学]"},{name:"2. 哈尔滨工业大学 (HIT)",workplace:"深圳, 中国",topic:"研究课题: 视频分析与理解",title:"研究实习生",supervisor:" 指导教授: 廖清教授 [哈尔滨工业大学(深圳)]"},{name:"3. 多伦多大学 (UofT)",workplace:"多伦多, 加拿大",topic:"研究课题: 数学与机器学习",title:"项目学生",supervisor:" 指导教授: Lvy Wang教授 [多伦多大学]"}],researchTitle:"研究兴趣",overallField:"整体领域",researchOverInterest:["计算机视觉 (CV)","机器学习 (ML)","第一人称视觉 (FPV)","神经科学 (NS)"],specialField:"特别兴趣",researchSpecialInterest:["1. 自我中心的视频理解和分析","2. 脑启发式人工智能","3. 动作识别","4. 人手姿态分析","5. 具身人工智能",".........................."],awardsTitle:"荣誉奖项",scholarship:"所获奖学金",scholar1:"1. 日本东京大学奖学金",scholar1explain:"[2年项目， 硕士期间全额支助]",scholar1supp:"(2023年期间仅22名学生获得该奖学金)",scholar2:"2. 中华人民共和国国家奖学金",scholar2explain:"[中华人民共和国教育部]",scholar2supp:"(中国排名前0.1%的学生)",scholar3:"3. 岭南学术奖学金  [杰出的学术代表]",scholar4:"4. 2021年学年度一等奖奖学金   [年级GPA第一名，2021年]",scholar5:"5. 高伟光企业奖学金   [杰出的工程代表]",scholar6:"6. 2020年学年度一等奖奖学金   [年级GPA第一名，2020年]",scholar7:"7. 2019年学年度一等奖奖学金   [年级GPA第一名，2019年]",awards:[{subtitle:"奖励",content:["优秀本科生的论文","美国数学建模竞赛国际二等奖","中国计算机学会AI视觉算法大赛 (排名13/2207)","中国人工智能电子设计大赛三等奖","新加坡国立大学荣誉奖","华为优秀开发者奖"]}],conferenceTitle:"论文发表",conferencePublication:[{name:"EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022 Technical Report",author:"<Strong>Nie Lin</Strong>, <a href='https://cai-mj.github.io/' target=\"_blank\">Minjie Cai</a><sup>✉</sup>",match:"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR-EPIC), 2022",match2:"",paper:"[论文]",projectPage:"[项目页面]",code:"[Github代码]",video:"[视频]"},{name:"Knowledge Condensation Distillation",author:"Chenxin Li, <a href='https://lmbxmu.github.io/'>Mingbao Lin</a>, Zhiyuan Ding, <Strong>Nie Lin</Strong>, Yihong Zhuang, <a href='https://huangyue05.github.io/'>Yue Huang</a><sup>✉</sup>,...",match:"European Conference on Computer Vision (ECCV), 2022",match2:"",paper:"[论文]",projectPage:"",code:"[Github代码]",video:""},{name:"Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects",author:"Zicong Fan, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Linlin Yang, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Zheng Liu, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao",match:"European Conference on Computer Vision (ECCV), 2024",match2:"",paper:"[论文]",projectPage:"[项目页面]"},{name:"HandCLR: Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on Large-Scale Hand Images in the Wild",author:"<Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Mingfang Zhang, Yifei Huang, Ryosuke Furuta, <a href='https://sites.google.com/ut-vision.org/ysato/' target=\"_blank\">Yoichi Sato</a>",match:"HANDS, European Conference on Computer Vision Worshop(ECCVW), 2024",match2:"",paper:"[论文]",projectPage:"[项目页面]"},{name:"Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",author:"Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://www.ms.k.u-tokyo.ac.jp/sugi/' target=\"_blank\">Masashi Sugiyama</a><sup>✉</sup>",match:"The Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024",match2:"",paper:"[论文]",code:"[Github代码]"},{name:"Translating Mental Imaginations into Characters with Codebooks and Dynamics-Enhanced Decoding",author:"Jingyuan Li, <a href='https://victorywys.github.io/' target=\"_blank\">Yansen Wang</a>, <Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <a href='https://recmind.cn/' target=\"_blank\">Dongsheng Li</a><sup>✉</sup>",match:"IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2025",match2:"",paper:"[论文]",code:"[Github代码 (Coming soon)]"},{name:"SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training",author:"<Strong><a href='https://lin-nie.github.io/' target=\"_blank\">Nie Lin</a></Strong>, <Strong><a href='https://tkhkaeio.github.io/' target=\"_blank\">Takehiko Ohkawa</a>, Mingfang Zhang, Yifei Huang, <a href='https://cai-mj.github.io/' target=\"_blank\">Minjie Cai</a>, Ming Li, Ryosuke Furuta, <a href='https://sites.google.com/ut-vision.org/ysato/' target=\"_blank\">Yoichi Sato</a><sup>✉</sup>",match:"The Thirteenth International Conference on Learning Representations (ICLR), 2025",match2:"",paper:"[论文]",projectPage:"[项目页面 (Coming soon)]",code:"[Github代码]",openreview:"[Openreview]"}],journalTitle:"期刊论文",journalPublication:[{name:"暂无"}],patentTitle:"发表专利",patent:[{name:"基于模拟信号处理的人机交互感知设备",author:'<a href="#">林涅</a>',number:"CN 202046227225.2"}],copyrghtTitle:"软件著作",softwareCopyrght:[{name:"日文邮政支付票据 OCR识别安卓客户端App（日文邮政票 OCR App）",Number:"软著登字第 A0003976号"},{name:"基于人工智能的视频版权保护系统",Number:"软著登字第 4840268号"}],projectsTitle:"项目经验",projectsHightLight:"高亮",projectsNote:" 表示最近开展的项目。",recentlyProjects:[{name:"EgoV: 一个全新的跨越真实和虚拟的以自我中心视频的数据集",author:"<strong>！！作为一个全新的长期项目，将会在我硕士阶段展开！！</strong>",match:"",match2:"",paper:"[研究计划书]",projectPage:"[项目页面]",code:"[Github代码]",video:"[视频]"},{name:"基于自中心视频手部区域的无监督域适应第一人称动作识别",author:"<strong>林 涅</strong>, 蔡 敏捷",match:"湖南大学",match2:"",paper:"[论文]",projectPage:"[项目页面]",code:"[Github代码]",video:"[视频]",photo:"[会议照片]"},{name:"基于迁移学习的知识浓缩蒸馏",author:"李 宸鑫, 林 明宝, <strong>林 涅</strong>, 庄毅鸿, 黄悦",match:"厦门大学 & 湖南大学",paper:"[论文]",projectPage:"[项目页面]",code:"[Github代码]",video:"[视频]"},{name:"第一人称视角下的数据集采集 (通过头戴式摄像头)",author:"<strong>林 涅</strong>",match:"湖南大学",paper:"",projectPage:"",code:"",video:""}],pastProjects:[{name:"多模态视频分析与理解",author:"<strong>林 涅</strong>, 郭 凡, 王 杰, 丁 烨, 廖 清",match:"哈尔滨工业大学 (HIT) ",match2:"",paper:"",projectPage:"",code:"",video:"[video]"},{name:"基于OpenMV机器视觉的手球动作识别控制系统",author:"<strong>林 涅</strong>, 王 斌, 周 清峰",match:"中国人工智能电子设计大赛",match2:"",paper:"",projectPage:"",code:"",video:""}],scientificFundTitle:"研究基金",scientificFund:[{name:'<a href="https://projectdb.jst.go.jp/grant/JST-PROJECT-23837230/">构建以人为中心的视觉与媒体技术国际联合研究网络</a>',match:"JST ASPIRE Grant Number JPMJAP2303"},{name:'<a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-24K02956/">基于影像的人物行为理解的语义深化</a>',match:"JSPS KAKENHI Grant Number JP24K02956,"},{name:'<a href="https://kaken.nii.ac.jp/en/grant/KAKENHI-PROJECT-22KF0119/">通过第一人称视角教学影像的多模态分析进行人物行为的学习与预测</a>',match:"JSPS KAKENHI Grant Number JP22KF0119,"},{name:'<a href="https://www.jst.go.jp/kisoken/act-x/en/project/111F001/111F001_2020.html">利用模仿型人工智能理解人类行为</a>',match:"JST ACT-X Grant Number JPMJAX2007"}],footer:{period:"© 2018 - 2025   林涅",lastUpdated:"上次更新: 2025年02月13日"}};var gt=pt;a["a"].use(it["a"]);var lt=new it["a"].Store({state:{words:st,dictionary:{en:st,zh:gt,jp:ht}},mutations:{SET_LANGUAGE(e,t){e.words=e.dictionary[t]}},actions:{setLanguage({commit:e},t){e("SET_LANGUAGE",t)}},getters:{words(e){return e.words}},modules:{}}),ut=(n("63bf"),n("ecee")),dt=n("c074"),ft=n("b702"),mt=n("f2d1"),wt=n("ad3d");ut["c"].add(dt["a"],ft["a"],mt["a"]),a["a"].component("font-awesome-icon",wt["a"]),a["a"].component("font-awesome-layers",wt["b"]),a["a"].component("font-awesome-layers-text",wt["c"]),a["a"].config.productionTip=!1,new a["a"]({store:lt,render:e=>e(ot)}).$mount("#app")},d12b:function(e,t,n){"use strict";n("94db")},d9ea:function(e,t,n){"use strict";n("aa33")},e466:function(e,t,n){var a={"./0.png":"0554","./1.png":"064c","./2.png":"c7b0","./3.png":"1ab2","./4.png":"0b40","./5.png":"9278","./6.png":"0fa9"};function o(e){var t=i(e);return n(t)}function i(e){if(!n.o(a,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return a[e]}o.keys=function(){return Object.keys(a)},o.resolve=i,e.exports=o,o.id="e466"},f179:function(e,t,n){},f5f6:function(e,t,n){e.exports=n.p+"img/LINNIE_10062023_japan.5cd844b9.jpg"},f6ca:function(e,t,n){"use strict";n("f179")}});
//# sourceMappingURL=app.d640f872.js.map